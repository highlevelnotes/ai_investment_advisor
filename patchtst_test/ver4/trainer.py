import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
import numpy as np
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import os

class BitcoinTrainer:
    def __init__(self, model, class_weights=None, device=None):
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = model.to(self.device)
        
        # ì†ì‹¤ í•¨ìˆ˜
        if class_weights is not None:
            class_weights = class_weights.to(self.device)
        self.criterion = nn.CrossEntropyLoss(weight=class_weights)
        
        # ì˜µí‹°ë§ˆì´ì €
        self.optimizer = optim.AdamW(
            self.model.parameters(), 
            lr=1e-4, 
            weight_decay=1e-5
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='max', factor=0.5, patience=10, verbose=True
        )
        
        # ê¸°ë¡
        self.train_losses = []
        self.val_accuracies = []
        self.best_val_acc = 0.0
        
        print(f"ðŸš€ íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™” ì™„ë£Œ (Device: {self.device})")
    
    def train_epoch(self, train_loader):
        """í•œ ì—í¬í¬ í•™ìŠµ - ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬"""
        self.model.train()
        total_loss = 0
        all_preds = []
        all_labels = []
        successful_batches = 0
        
        pbar = tqdm(train_loader, desc="Training")
        for batch_idx, (sequences, labels) in enumerate(pbar):
            try:
                sequences = sequences.to(self.device)
                labels = labels.to(self.device)
                
                # ìž…ë ¥ ê²€ì¦
                if sequences.size(0) == 0 or labels.size(0) == 0:
                    print(f"âš ï¸ ë¹ˆ ë°°ì¹˜ ê±´ë„ˆëœ€: {batch_idx}")
                    continue
                
                # Forward pass
                self.optimizer.zero_grad()
                logits = self.model(sequences)
                loss = self.criterion(logits, labels)
                
                # Backward pass
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()
                
                # ê¸°ë¡
                total_loss += loss.item()
                successful_batches += 1
                
                preds = torch.argmax(logits, dim=1)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                if len(all_labels) > 0:
                    current_acc = accuracy_score(all_labels, all_preds)
                    pbar.set_postfix({
                        'Loss': f'{loss.item():.4f}',
                        'Acc': f'{current_acc:.4f}'
                    })
                
            except Exception as e:
                print(f"âš ï¸ ë°°ì¹˜ {batch_idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        if successful_batches == 0:
            print("âŒ ì„±ê³µí•œ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return 0.0, 0.0
        
        avg_loss = total_loss / successful_batches
        accuracy = accuracy_score(all_labels, all_preds) if len(all_labels) > 0 else 0.0
        
        return avg_loss, accuracy
    
    def validate(self, val_loader):
        """ê²€ì¦ - ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬"""
        self.model.eval()
        total_loss = 0
        all_preds = []
        all_labels = []
        successful_batches = 0
        
        with torch.no_grad():
            for batch_idx, (sequences, labels) in enumerate(tqdm(val_loader, desc="Validation")):
                try:
                    sequences = sequences.to(self.device)
                    labels = labels.to(self.device)
                    
                    if sequences.size(0) == 0 or labels.size(0) == 0:
                        continue
                    
                    logits = self.model(sequences)
                    loss = self.criterion(logits, labels)
                    
                    total_loss += loss.item()
                    successful_batches += 1
                    
                    preds = torch.argmax(logits, dim=1)
                    all_preds.extend(preds.cpu().numpy())
                    all_labels.extend(labels.cpu().numpy())
                    
                except Exception as e:
                    print(f"âš ï¸ ê²€ì¦ ë°°ì¹˜ {batch_idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
        
        if successful_batches == 0:
            return 0.0, 0.0, [], []
        
        avg_loss = total_loss / successful_batches
        accuracy = accuracy_score(all_labels, all_preds) if len(all_labels) > 0 else 0.0
        
        return avg_loss, accuracy, all_preds, all_labels
    
    def train(self, train_loader, val_loader, epochs=100, save_path='models/best_model.pth'):
        """ì „ì²´ í•™ìŠµ ê³¼ì • - ê°•í™”ëœ ì—ëŸ¬ ì²˜ë¦¬"""
        print(f"ðŸŽ¯ í•™ìŠµ ì‹œìž‘ (ì—í¬í¬: {epochs})")
        
        for epoch in range(epochs):
            print(f"\n=== Epoch {epoch+1}/{epochs} ===")
            
            try:
                # í•™ìŠµ
                train_loss, train_acc = self.train_epoch(train_loader)
                self.train_losses.append(train_loss)
                
                # ê²€ì¦
                val_loss, val_acc, val_preds, val_labels = self.validate(val_loader)
                self.val_accuracies.append(val_acc)
                
                # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                self.scheduler.step(val_acc)
                
                print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
                print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
                
                # ìµœê³  ëª¨ë¸ ì €ìž¥
                if val_acc > self.best_val_acc:
                    self.best_val_acc = val_acc
                    self.save_model(save_path)
                    print(f"ðŸŽ‰ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ìž¥! (Acc: {val_acc:.4f})")
                    
                    # ìƒì„¸ ë¶„ë¥˜ ë¦¬í¬íŠ¸ (ì—ëŸ¬ ì²˜ë¦¬)
                    if len(val_labels) > 0 and len(val_preds) > 0:
                        try:
                            print("\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
                            print(classification_report(val_labels, val_preds, 
                                                      target_names=['ë³´í•©', 'ìƒìŠ¹', 'í•˜ë½'],
                                                      zero_division=0))
                        except Exception as e:
                            print(f"ë¶„ë¥˜ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
                
                # ì¡°ê¸° ì¢…ë£Œ
                if epoch > 20 and len(self.val_accuracies) >= 10:
                    recent_avg = np.mean(self.val_accuracies[-10:])
                    if val_acc < recent_avg * 0.95:
                        print("ì¡°ê¸° ì¢…ë£Œ!")
                        break
                        
            except Exception as e:
                print(f"âŒ ì—í¬í¬ {epoch+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        print(f"âœ… í•™ìŠµ ì™„ë£Œ! ìµœê³  ê²€ì¦ ì •í™•ë„: {self.best_val_acc:.4f}")
    
    def save_model(self, path):
        """ëª¨ë¸ ì €ìž¥"""
        try:
            os.makedirs(os.path.dirname(path), exist_ok=True)
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'best_val_acc': self.best_val_acc,
                'config': self.model.config
            }, path)
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ ì €ìž¥ ì‹¤íŒ¨: {e}")
    
    def load_model(self, path):
        """ëª¨ë¸ ë¡œë“œ"""
        try:
            checkpoint = torch.load(path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.best_val_acc = checkpoint['best_val_acc']
            print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (ìµœê³  ì •í™•ë„: {self.best_val_acc:.4f})")
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
