import torch
import torch.nn as nn
from transformers import PatchTSTForPrediction, PatchTSTConfig
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader
import warnings
warnings.filterwarnings('ignore')

class TransferLearningPatchTST(nn.Module):
    """ì‹¤ì œ ì‚¬ì „í›ˆë ¨ëœ PatchTST ëª¨ë¸ì„ ì‚¬ìš©í•œ ì „ì´í•™ìŠµ"""
    
    def __init__(self, config, pretrained_model_name="microsoft/patchtst-etth1-forecast"):
        super().__init__()
        self.config = config
        
        # ì‚¬ì „í›ˆë ¨ëœ PatchTST ëª¨ë¸ ë¡œë“œ
        print(f"ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ ì¤‘: {pretrained_model_name}")
        try:
            self.patchtst_config = PatchTSTConfig.from_pretrained(pretrained_model_name)
            self.backbone = PatchTSTForPrediction.from_pretrained(pretrained_model_name)
            print("âœ… ì‚¬ì „í›ˆë ¨ëœ PatchTST ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
        except Exception as e:
            print(f"ì‚¬ì „í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ê¸°ë³¸ PatchTST ì„¤ì •ìœ¼ë¡œ ì´ˆê¸°í™”...")
            self.patchtst_config = PatchTSTConfig(
                context_length=config['sequence_length'],
                prediction_length=1,  # ë¶„ë¥˜ë¥¼ ìœ„í•´ 1ë¡œ ì„¤ì •
                num_input_channels=config['num_features'],
                patch_length=16,
                patch_stride=8,
                d_model=128,
                num_attention_heads=16,
                num_hidden_layers=3,
                ffn_dim=256,
                dropout=0.1
            )
            self.backbone = PatchTSTForPrediction(self.patchtst_config)
        
        # ë°±ë³¸ ëª¨ë¸ì˜ ì¼ë¶€ ë ˆì´ì–´ ë™ê²°
        self.freeze_backbone_layers()
        
        # ë¶„ë¥˜ë¥¼ ìœ„í•œ ìƒˆë¡œìš´ í—¤ë“œ ì¶”ê°€
        self.classification_head = nn.Sequential(
            nn.Linear(self.patchtst_config.d_model, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(64, config['num_classes'])
        )
        
        # ì „ì—­ í‰ê·  í’€ë§
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
    def freeze_backbone_layers(self, freeze_ratio=0.7):
        """ë°±ë³¸ ëª¨ë¸ì˜ ì¼ë¶€ ë ˆì´ì–´ ë™ê²°"""
        total_layers = len(list(self.backbone.named_parameters()))
        freeze_count = int(total_layers * freeze_ratio)
        
        frozen_count = 0
        for name, param in self.backbone.named_parameters():
            if frozen_count < freeze_count:
                param.requires_grad = False
                frozen_count += 1
            else:
                param.requires_grad = True
        
        trainable_params = sum(p.numel() for p in self.backbone.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.backbone.parameters())
        
        print(f"ë°±ë³¸ ëª¨ë¸ ë™ê²°: {freeze_ratio*100:.1f}% ({frozen_count}/{total_layers} ë ˆì´ì–´)")
        print(f"í›ˆë ¨ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,} / {total_params:,}")
    
    def forward(self, x):
        # x: (batch_size, sequence_length, num_features)
        batch_size = x.size(0)
        
        # PatchTSTëŠ” (batch_size, num_channels, sequence_length) í˜•íƒœë¥¼ ê¸°ëŒ€
        x_transposed = x.transpose(1, 2)  # (batch_size, num_features, sequence_length)
        
        try:
            # ì‚¬ì „í›ˆë ¨ëœ PatchTST ë°±ë³¸ í†µê³¼
            outputs = self.backbone(past_values=x_transposed)
            
            # ì˜ˆì¸¡ ê²°ê³¼ì—ì„œ íŠ¹ì„± ì¶”ì¶œ
            if hasattr(outputs, 'prediction'):
                features = outputs.prediction  # (batch_size, prediction_length, num_channels)
            else:
                features = outputs.last_hidden_state  # ë˜ëŠ” ë‹¤ë¥¸ ì¶œë ¥
            
            # íŠ¹ì„±ì„ 1ì°¨ì›ìœ¼ë¡œ ë³€í™˜
            if len(features.shape) == 3:
                features = features.mean(dim=1)  # ì‹œê°„ ì°¨ì› í‰ê· 
                features = features.mean(dim=1)  # ì±„ë„ ì°¨ì› í‰ê· 
            elif len(features.shape) == 2:
                features = features.mean(dim=1)  # ë§ˆì§€ë§‰ ì°¨ì› í‰ê· 
            
            # 1ì°¨ì› ë²¡í„°ë¡œ ë§Œë“¤ê¸°
            if len(features.shape) > 1:
                features = features.view(batch_size, -1)
                features = features.mean(dim=1, keepdim=True)
            
            # ë¶„ë¥˜ í—¤ë“œ í†µê³¼
            if features.dim() == 1:
                features = features.unsqueeze(1)
            
            # ê³ ì • í¬ê¸° íŠ¹ì„±ìœ¼ë¡œ ë³€í™˜
            if features.size(1) != self.patchtst_config.d_model:
                features = torch.nn.functional.adaptive_avg_pool1d(
                    features.unsqueeze(1), self.patchtst_config.d_model
                ).squeeze(1)
            
            logits = self.classification_head(features)
            
        except Exception as e:
            print(f"PatchTST í¬ì›Œë“œ ì—ëŸ¬: {e}")
            # í´ë°±: ê°„ë‹¨í•œ íŠ¹ì„± ì¶”ì¶œ
            features = x.mean(dim=1)  # ì‹œê°„ ì°¨ì› í‰ê· 
            features = torch.nn.functional.adaptive_avg_pool1d(
                features.unsqueeze(1), self.patchtst_config.d_model
            ).squeeze(1)
            logits = self.classification_head(features)
        
        return logits

class FineTuningTrainer:
    """ì „ì´í•™ìŠµ ì „ìš© íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, model, class_weights=None, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.model = model.to(device)
        self.device = device
        
        # ê°€ì¤‘ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤
        if class_weights is not None:
            class_weights = class_weights.to(device)
        self.criterion = nn.CrossEntropyLoss(weight=class_weights)
        
        # ì°¨ë³„ì  í•™ìŠµë¥  ì ìš©
        backbone_params = []
        head_params = []
        
        for name, param in model.named_parameters():
            if param.requires_grad:
                if 'classification_head' in name:
                    head_params.append(param)
                else:
                    backbone_params.append(param)
        
        # ë°±ë³¸ì€ ë‚®ì€ í•™ìŠµë¥ , í—¤ë“œëŠ” ë†’ì€ í•™ìŠµë¥ 
        self.optimizer = torch.optim.AdamW([
            {'params': backbone_params, 'lr': 1e-5},  # ë°±ë³¸: ë§¤ìš° ë‚®ì€ í•™ìŠµë¥ 
            {'params': head_params, 'lr': 1e-3}       # í—¤ë“œ: ë†’ì€ í•™ìŠµë¥ 
        ], weight_decay=1e-4)
        
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer, mode='max', factor=0.5, patience=5, verbose=True
        )
        
        self.best_val_acc = 0.0
        self.train_losses = []
        self.val_accuracies = []
    
    def train_epoch(self, train_loader):
        """í•œ ì—í¬í¬ ì „ì´í•™ìŠµ"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (sequences, labels) in enumerate(train_loader):
            sequences = sequences.to(self.device)
            labels = labels.to(self.device)
            
            self.optimizer.zero_grad()
            
            try:
                logits = self.model(sequences)
                loss = self.criterion(logits, labels)
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()
                
                total_loss += loss.item()
                
                # ì •í™•ë„ ê³„ì‚°
                _, predicted = torch.max(logits.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
                
                if batch_idx % 10 == 0:
                    print(f'Batch {batch_idx}, Loss: {loss.item():.4f}, Acc: {100.*correct/total:.2f}%')
                    
            except Exception as e:
                print(f"ë°°ì¹˜ {batch_idx} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        avg_loss = total_loss / len(train_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def validate(self, val_loader):
        """ê²€ì¦"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for sequences, labels in val_loader:
                sequences = sequences.to(self.device)
                labels = labels.to(self.device)
                
                try:
                    logits = self.model(sequences)
                    loss = self.criterion(logits, labels)
                    
                    total_loss += loss.item()
                    
                    _, predicted = torch.max(logits.data, 1)
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()
                    
                except Exception as e:
                    print(f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
        
        avg_loss = total_loss / len(val_loader)
        accuracy = 100. * correct / total
        
        return avg_loss, accuracy
    
    def fine_tune(self, train_loader, val_loader, epochs=50, save_path='models/transfer_learning/best_model.pth'):
        """ì „ì´í•™ìŠµ ì‹¤í–‰"""
        print("ğŸš€ PatchTST ì „ì´í•™ìŠµ ì‹œì‘!")
        
        for epoch in range(epochs):
            print(f"\n=== Epoch {epoch+1}/{epochs} ===")
            
            # í•™ìŠµ
            train_loss, train_acc = self.train_epoch(train_loader)
            self.train_losses.append(train_loss)
            
            # ê²€ì¦
            val_loss, val_acc = self.validate(val_loader)
            self.val_accuracies.append(val_acc)
            
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
            self.scheduler.step(val_acc)
            
            print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%")
            print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")
            
            # ìµœê³  ëª¨ë¸ ì €ì¥
            if val_acc > self.best_val_acc:
                self.best_val_acc = val_acc
                self.save_model(save_path)
                print(f"ğŸ‰ ìƒˆë¡œìš´ ìµœê³  ëª¨ë¸ ì €ì¥! Val Acc: {val_acc:.2f}%")
            
            # ì¡°ê¸° ì¢…ë£Œ
            if epoch > 10 and val_acc < np.mean(self.val_accuracies[-5:]) * 0.95:
                print("ì¡°ê¸° ì¢…ë£Œ!")
                break
        
        print(f"âœ… ì „ì´í•™ìŠµ ì™„ë£Œ! ìµœê³  ê²€ì¦ ì •í™•ë„: {self.best_val_acc:.2f}%")
    
    def save_model(self, path):
        """ëª¨ë¸ ì €ì¥"""
        import os
        os.makedirs(os.path.dirname(path), exist_ok=True)
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_val_acc': self.best_val_acc,
            'config': self.model.config
        }, path)
    
    def load_model(self, path):
        """ëª¨ë¸ ë¡œë“œ"""
        checkpoint = torch.load(path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.best_val_acc = checkpoint['best_val_acc']
        print(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ìµœê³  ê²€ì¦ ì •í™•ë„: {self.best_val_acc:.2f}%")
